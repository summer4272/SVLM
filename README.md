# Small Vision Language Model (SVLM)

## ğŸ— æ¨¡å‹ç»“æ„ç¤ºæ„ (Model Architecture)
![æ¨¡å‹ç»“æ„](Image/MODEL.png)

---
## åº–ä¸è§£ç‰› (Starting small, thinking big)

æœ¬é¡¹ç›®ä»é›¶å¼€å§‹æ‰‹æ“è§†è§‰-æ–‡æœ¬ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œ**æ˜¾å­˜ 8GB å³å¯è®­ç»ƒ**ï¼Œé€‚åˆåˆå­¦è€…å’Œä½ç®—åŠ›äººç¾¤è¿›è¡Œå®è·µä¸å­¦ä¹ ã€‚  
This project builds a **vision-language unified multimodal model from scratch**, requiring **only 8GB GPU memory** to train. It is designed for beginners and users with limited resources.  

ğŸ“Œ **ç‰¹ç‚¹ / Features:**  
- ğŸ”§ ä»é›¶å®ç° / Implemented from scratch  
- ğŸ’» ä½ç®—åŠ›å¯è·‘ / Runs on limited resources  
- ğŸ“š é€‚åˆå­¦ä¹ ä¸å®è·µ / Suitable for learning & practice  
- ğŸŒ ç®€æ´é«˜æ•ˆ / Compact & efficient  

---
## ğŸ“· æ•ˆæœå±•ç¤º (Project Showcase)

![ç»“æœæ¼”ç¤º](Image/videotogif.gif)

---

## ğŸ“ˆ è®­ç»ƒæ•ˆæœæ›²çº¿ (Training Curves)

![è®­ç»ƒæ›²çº¿](Image/train.png)

---
## ğŸš€ å¿«é€Ÿå¼€å§‹ (Quickstart)

åªéœ€å‡ è¡Œä»£ç å³å¯è¿è¡Œä¸€ä¸ªç®€å•çš„æ¨ç†ç¤ºä¾‹ï¼š  
Run a simple inference demo in just a few lines:

```python
from svlm import SVLMModel

# åˆå§‹åŒ–æ¨¡å‹
model = SVLMModel.from_pretrained("YourModelPath")

# è¾“å…¥å›¾åƒå’Œæ–‡æœ¬
image = "Image/demo.png"
text = "What is in the picture?"

# æ‰§è¡Œæ¨ç†
output = model.predict(image, text)

print("Result:", output)
